{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shtdbb/pytorch-cifar100/blob/master/examples/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYHhZ24vzMe1",
        "outputId": "541b0ccd-be05-4cf9-9f8d-81b37c2eb7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatGLM-Tuning'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 137 (delta 41), reused 33 (delta 33), pack-reused 70\u001b[K\n",
            "Receiving objects: 100% (137/137), 8.02 MiB | 9.63 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/ChatGLM-Tuning\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 14))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-01ir0yn6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-01ir0yn6\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 098962fa6515f2e4fe83a757f5995d3ffbb1c373\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes==0.37.1\n",
            "  Downloading bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.17.1\n",
            "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<3.20.1,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (3.19.6)\n",
            "Collecting transformers==4.27.1\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting icetk\n",
            "  Downloading icetk-0.0.7-py3-none-any.whl (16 kB)\n",
            "Collecting cpm_kernels==1.0.11\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 KB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (1.13.1+cu116)\n",
            "Collecting datasets==2.10.1\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (1.4.4)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (2023.3.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from icetk->-r requirements.txt (line 8)) (0.14.1+cu116)\n",
            "Collecting icetk\n",
            "  Downloading icetk-0.0.6-py3-none-any.whl (15 kB)\n",
            "  Downloading icetk-0.0.5-py3-none-any.whl (15 kB)\n",
            "  Downloading icetk-0.0.4-py3-none-any.whl (15 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (4.5.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 13)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->icetk->-r requirements.txt (line 8)) (8.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1->-r requirements.txt (line 13)) (1.16.0)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=40706 sha256=25db7e6c8bb1e3d43061d79ae584590fb7ddfbc826d901fbaea9aa582a914538\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a0vkxrcr/wheels/2d/60/1b/0edd9dc0f0c489738b1166bc1b0b560ee368f7721f89d06e3a\n",
            "Successfully built peft\n",
            "Installing collected packages: tokenizers, sentencepiece, cpm_kernels, bitsandbytes, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, accelerate, transformers, icetk, aiohttp, peft, datasets\n",
            "Successfully installed accelerate-0.17.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.37.1 cpm_kernels-1.0.11 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.3 icetk-0.0.4 multidict-6.0.4 multiprocess-0.70.14 peft-0.3.0.dev0 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.1 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mymusise/ChatGLM-Tuning.git\n",
        "%cd  ChatGLM-Tuning\n",
        "!pip install -r requirements.txt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTTJ3af1zMe7",
        "outputId": "3e4f85e5-45a2-4357-fce7-b7e6c8cfd697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "formatting..: 100% 52002/52002 [00:00<00:00, 190912.01it/s]\n"
          ]
        }
      ],
      "source": [
        "!python cover_alpaca2jsonl.py \\\n",
        "    --data_path data/alpaca_data.json \\\n",
        "    --save_path data/alpaca_data.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkjB-zrBzMe8",
        "outputId": "f716035e-68f5-4d46-d5aa-5676089bca04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-eb6b821bd81cdfce/0.0.0...\n",
            "Generating train split: 0 examples [00:00, ? examples/s]\n",
            "Downloading (…)okenizer_config.json: 100% 416/416 [00:00<00:00, 61.0kB/s]\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "\n",
            "Downloading (…)enization_chatglm.py: 100% 12.5k/12.5k [00:00<00:00, 1.90MB/s]\n",
            "\n",
            "Downloading ice_text.model: 100% 2.70M/2.70M [00:00<00:00, 80.8MB/s]\n",
            "\n",
            "Downloading (…)lve/main/config.json: 100% 697/697 [00:00<00:00, 238kB/s]\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "\n",
            "Downloading (…)iguration_chatglm.py: 100% 3.88k/3.88k [00:00<00:00, 1.53MB/s]\n",
            "\n",
            "Generating train split: 1 examples [00:17, 17.26s/ examples]\n",
            "Generating train split: 185 examples [00:17, 15.17 examples/s]\n",
            "Generating train split: 457 examples [00:17, 46.25 examples/s]\n",
            "Generating train split: 711 examples [00:17, 85.94 examples/s]\n",
            "Generating train split: 900 examples [00:17, 126.41 examples/s]\n",
            "  2% 903/52002 [00:00<00:28, 1804.52it/s]\u001b[A\n",
            "Generating train split: 1097 examples [00:17, 179.33 examples/s]\n",
            "Generating train split: 1279 examples [00:18, 247.43 examples/s]\n",
            "Generating train split: 1449 examples [00:18, 328.69 examples/s]\n",
            "Generating train split: 1619 examples [00:18, 430.09 examples/s]\n",
            "Generating train split: 1796 examples [00:18, 557.65 examples/s]\n",
            "Generating train split: 1977 examples [00:18, 707.32 examples/s]\n",
            "Generating train split: 2213 examples [00:18, 882.66 examples/s]\n",
            "Generating train split: 2400 examples [00:18, 1042.06 examples/s]\n",
            "Generating train split: 2593 examples [00:18, 1208.00 examples/s]\n",
            "Generating train split: 2782 examples [00:18, 1347.88 examples/s]\n",
            "Generating train split: 3041 examples [00:19, 1463.36 examples/s]\n",
            "  6% 3051/52002 [00:01<00:27, 1772.68it/s]\u001b[A\n",
            "Generating train split: 3297 examples [00:19, 1536.47 examples/s]\n",
            "Generating train split: 3560 examples [00:19, 1596.62 examples/s]\n",
            "  7% 3585/52002 [00:02<00:27, 1730.19it/s]\u001b[A\n",
            "Generating train split: 3814 examples [00:19, 1624.72 examples/s]\n",
            "Generating train split: 3995 examples [00:19, 1664.54 examples/s]\n",
            "Generating train split: 4262 examples [00:19, 1699.79 examples/s]\n",
            "Generating train split: 4444 examples [00:19, 1727.08 examples/s]\n",
            "Generating train split: 4630 examples [00:19, 1759.46 examples/s]\n",
            "Generating train split: 4816 examples [00:20, 1780.62 examples/s]\n",
            "Generating train split: 5062 examples [00:20, 1724.04 examples/s]\n",
            "Generating train split: 5241 examples [00:20, 1735.18 examples/s]\n",
            "Generating train split: 5428 examples [00:20, 1768.86 examples/s]\n",
            "Generating train split: 5611 examples [00:20, 1780.02 examples/s]\n",
            " 11% 5613/52002 [00:03<00:25, 1790.89it/s]\u001b[A\n",
            "Generating train split: 5871 examples [00:20, 1757.60 examples/s]\n",
            "Generating train split: 6129 examples [00:20, 1737.01 examples/s]\n",
            " 12% 6149/52002 [00:03<00:26, 1715.64it/s]\u001b[A\n",
            "Generating train split: 6345 examples [00:20, 1630.19 examples/s]\n",
            "Generating train split: 6546 examples [00:21, 1383.61 examples/s]\n",
            "Generating train split: 6745 examples [00:21, 1242.98 examples/s]\n",
            " 13% 6764/52002 [00:04<00:38, 1173.49it/s]\u001b[A\n",
            "Generating train split: 6899 examples [00:21, 1174.16 examples/s]\n",
            "Generating train split: 7022 examples [00:21, 1073.14 examples/s]\n",
            "Generating train split: 7168 examples [00:21, 1040.36 examples/s]\n",
            " 14% 7211/52002 [00:04<00:44, 1003.10it/s]\u001b[A\n",
            "Generating train split: 7320 examples [00:21, 1027.71 examples/s]\n",
            "Generating train split: 7449 examples [00:22, 975.33 examples/s] \n",
            "Generating train split: 7573 examples [00:22, 929.96 examples/s]\n",
            "Generating train split: 7684 examples [00:22, 964.33 examples/s]\n",
            "Generating train split: 7785 examples [00:22, 972.17 examples/s]\n",
            "Generating train split: 7906 examples [00:22, 909.34 examples/s]\n",
            "Generating train split: 8000 examples [00:22, 906.65 examples/s]\n",
            "Generating train split: 8104 examples [00:22, 938.31 examples/s]\n",
            " 16% 8104/52002 [00:05<00:47, 928.43it/s]\u001b[A\n",
            "Generating train split: 8242 examples [00:22, 927.81 examples/s]\n",
            "Generating train split: 8377 examples [00:23, 911.47 examples/s]\n",
            "Generating train split: 8475 examples [00:23, 925.94 examples/s]\n",
            "Generating train split: 8578 examples [00:23, 951.11 examples/s]\n",
            "Generating train split: 8679 examples [00:23, 964.42 examples/s]\n",
            " 17% 8691/52002 [00:06<00:44, 978.20it/s]\u001b[A\n",
            "Generating train split: 8838 examples [00:23, 994.14 examples/s]\n",
            "Generating train split: 8948 examples [00:23, 1019.65 examples/s]\n",
            "Generating train split: 9075 examples [00:23, 957.09 examples/s] \n",
            "Generating train split: 9207 examples [00:24, 925.67 examples/s]\n",
            "Generating train split: 9309 examples [00:24, 947.98 examples/s]\n",
            " 18% 9309/52002 [00:06<00:44, 956.01it/s]\u001b[A\n",
            "Generating train split: 9424 examples [00:24, 996.89 examples/s]\n",
            "Generating train split: 9532 examples [00:24, 1014.27 examples/s]\n",
            "Generating train split: 9652 examples [00:24, 934.56 examples/s] \n",
            "Generating train split: 9749 examples [00:24, 941.21 examples/s]\n",
            "Generating train split: 9881 examples [00:24, 915.42 examples/s]\n",
            "Generating train split: 10000 examples [00:24, 974.46 examples/s]\n",
            "Generating train split: 10187 examples [00:24, 1205.84 examples/s]\n",
            "Generating train split: 10352 examples [00:25, 1325.34 examples/s]\n",
            "Generating train split: 10531 examples [00:25, 1452.15 examples/s]\n",
            "Generating train split: 10714 examples [00:25, 1557.82 examples/s]\n",
            "Generating train split: 10891 examples [00:25, 1617.13 examples/s]\n",
            " 21% 10953/52002 [00:08<00:25, 1628.72it/s]\u001b[A\n",
            "Generating train split: 11123 examples [00:25, 1586.63 examples/s]\n",
            "Generating train split: 11367 examples [00:25, 1598.99 examples/s]\n",
            "Generating train split: 11542 examples [00:25, 1633.16 examples/s]\n",
            "Generating train split: 11781 examples [00:25, 1612.90 examples/s]\n",
            "Generating train split: 11944 examples [00:25, 1613.21 examples/s]\n",
            " 23% 11971/52002 [00:08<00:24, 1613.13it/s]\u001b[A\n",
            "Generating train split: 12183 examples [00:26, 1605.04 examples/s]\n",
            "Generating train split: 12367 examples [00:26, 1661.12 examples/s]\n",
            "Generating train split: 12552 examples [00:26, 1706.41 examples/s]\n",
            "Generating train split: 12789 examples [00:26, 1654.54 examples/s]\n",
            "Generating train split: 12960 examples [00:26, 1665.92 examples/s]\n",
            "Generating train split: 13129 examples [00:26, 1669.26 examples/s]\n",
            "Generating train split: 13312 examples [00:26, 1712.01 examples/s]\n",
            "Generating train split: 13497 examples [00:26, 1748.05 examples/s]\n",
            "Generating train split: 13744 examples [00:27, 1705.65 examples/s]\n",
            "Generating train split: 13925 examples [00:27, 1726.19 examples/s]\n",
            " 27% 13931/52002 [00:09<00:22, 1707.09it/s]\u001b[A\n",
            "Generating train split: 14167 examples [00:27, 1646.51 examples/s]\n",
            "Generating train split: 14359 examples [00:27, 1712.87 examples/s]\n",
            "Generating train split: 14536 examples [00:27, 1722.76 examples/s]\n",
            "Generating train split: 14785 examples [00:27, 1698.37 examples/s]\n",
            "Generating train split: 14961 examples [00:27, 1711.96 examples/s]\n",
            " 29% 14976/52002 [00:10<00:21, 1703.82it/s]\u001b[A\n",
            "Generating train split: 15169 examples [00:27, 1583.89 examples/s]\n",
            "Generating train split: 15340 examples [00:27, 1610.58 examples/s]\n",
            "Generating train split: 15509 examples [00:28, 1627.67 examples/s]\n",
            "Generating train split: 15674 examples [00:28, 1631.62 examples/s]\n",
            "Generating train split: 15843 examples [00:28, 1646.33 examples/s]\n",
            "Generating train split: 16013 examples [00:28, 1658.36 examples/s]\n",
            "Generating train split: 16256 examples [00:28, 1641.57 examples/s]\n",
            "Generating train split: 16425 examples [00:28, 1652.10 examples/s]\n",
            "Generating train split: 16597 examples [00:28, 1667.72 examples/s]\n",
            "Generating train split: 16780 examples [00:28, 1710.23 examples/s]\n",
            "Generating train split: 17015 examples [00:29, 1652.64 examples/s]\n",
            " 33% 17041/52002 [00:11<00:21, 1644.00it/s]\u001b[A\n",
            "Generating train split: 17265 examples [00:29, 1654.21 examples/s]\n",
            "Generating train split: 17433 examples [00:29, 1658.23 examples/s]\n",
            "Generating train split: 17623 examples [00:29, 1717.51 examples/s]\n",
            "Generating train split: 17878 examples [00:29, 1703.08 examples/s]\n",
            " 34% 17921/52002 [00:12<00:20, 1671.72it/s]\u001b[A\n",
            "Generating train split: 18107 examples [00:29, 1638.43 examples/s]\n",
            "Generating train split: 18282 examples [00:29, 1438.28 examples/s]\n",
            "Generating train split: 18456 examples [00:30, 842.96 examples/s] \n",
            "Generating train split: 18658 examples [00:30, 950.33 examples/s]\n",
            "Generating train split: 18825 examples [00:30, 1072.44 examples/s]\n",
            "Generating train split: 19000 examples [00:30, 1184.12 examples/s]\n",
            "Generating train split: 19167 examples [00:30, 1286.48 examples/s]\n",
            "Generating train split: 19356 examples [00:30, 1425.74 examples/s]\n",
            " 37% 19387/52002 [00:13<00:25, 1266.78it/s]\u001b[A\n",
            "Generating train split: 19574 examples [00:31, 756.69 examples/s] \n",
            "Generating train split: 19750 examples [00:31, 900.62 examples/s]\n",
            "Generating train split: 19914 examples [00:31, 1025.74 examples/s]\n",
            "Generating train split: 20095 examples [00:31, 1157.79 examples/s]\n",
            "Generating train split: 20265 examples [00:31, 1272.62 examples/s]\n",
            "Generating train split: 20443 examples [00:31, 1389.37 examples/s]\n",
            "Generating train split: 20631 examples [00:32, 760.06 examples/s] \n",
            "Generating train split: 20836 examples [00:32, 955.78 examples/s]\n",
            "Generating train split: 20992 examples [00:32, 1063.31 examples/s]\n",
            "Generating train split: 21172 examples [00:32, 1211.99 examples/s]\n",
            "Generating train split: 21347 examples [00:32, 1330.07 examples/s]\n",
            "Generating train split: 21522 examples [00:32, 1430.80 examples/s]\n",
            "Generating train split: 21709 examples [00:32, 1541.65 examples/s]\n",
            " 42% 21732/52002 [00:15<00:18, 1596.81it/s]\u001b[A\n",
            "Generating train split: 21941 examples [00:33, 1540.92 examples/s]\n",
            "Generating train split: 22114 examples [00:33, 1587.47 examples/s]\n",
            "Generating train split: 22285 examples [00:33, 1615.93 examples/s]\n",
            "Generating train split: 22517 examples [00:33, 1587.40 examples/s]\n",
            "Generating train split: 22755 examples [00:33, 1582.91 examples/s]\n",
            " 44% 22757/52002 [00:16<00:18, 1599.83it/s]\u001b[A\n",
            "Generating train split: 22996 examples [00:33, 1587.96 examples/s]\n",
            "Generating train split: 23223 examples [00:33, 1558.84 examples/s]\n",
            "Generating train split: 23403 examples [00:34, 1609.51 examples/s]\n",
            "Generating train split: 23588 examples [00:34, 1663.44 examples/s]\n",
            "Generating train split: 23780 examples [00:34, 1727.17 examples/s]\n",
            " 46% 23795/52002 [00:17<00:16, 1761.48it/s]\u001b[A\n",
            "Generating train split: 24011 examples [00:34, 1658.06 examples/s]\n",
            "Generating train split: 24238 examples [00:34, 1602.94 examples/s]\n",
            "Generating train split: 24418 examples [00:34, 1647.83 examples/s]\n",
            "Generating train split: 24598 examples [00:34, 1682.23 examples/s]\n",
            "Generating train split: 24805 examples [00:34, 1399.29 examples/s]\n",
            " 48% 24811/52002 [00:17<00:20, 1354.68it/s]\u001b[A\n",
            "Generating train split: 24956 examples [00:35, 1274.59 examples/s]\n",
            "Generating train split: 25103 examples [00:35, 1160.27 examples/s]\n",
            "Generating train split: 25260 examples [00:35, 1109.92 examples/s]\n",
            "Generating train split: 25379 examples [00:35, 1009.70 examples/s]\n",
            " 49% 25419/52002 [00:18<00:27, 971.32it/s] \u001b[A\n",
            "Generating train split: 25536 examples [00:35, 1014.58 examples/s]\n",
            "Generating train split: 25682 examples [00:35, 1000.62 examples/s]\n",
            "Generating train split: 25816 examples [00:36, 958.04 examples/s] \n",
            " 50% 25829/52002 [00:18<00:27, 952.63it/s]\u001b[A\n",
            "Generating train split: 25950 examples [00:36, 934.54 examples/s]\n",
            "Generating train split: 26098 examples [00:36, 946.65 examples/s]\n",
            "Generating train split: 26194 examples [00:36, 945.80 examples/s]\n",
            "Generating train split: 26304 examples [00:36, 977.15 examples/s]\n",
            " 51% 26334/52002 [00:19<00:25, 988.63it/s]\u001b[A\n",
            "Generating train split: 26441 examples [00:36, 949.04 examples/s]\n",
            "Generating train split: 26588 examples [00:36, 955.83 examples/s]\n",
            "Generating train split: 26703 examples [00:37, 893.90 examples/s]\n",
            " 51% 26722/52002 [00:19<00:28, 873.74it/s]\u001b[A\n",
            "Generating train split: 26840 examples [00:37, 895.92 examples/s]\n",
            "Generating train split: 26937 examples [00:37, 911.14 examples/s]\n",
            "Generating train split: 27072 examples [00:37, 902.76 examples/s]\n",
            "Generating train split: 27173 examples [00:37, 926.79 examples/s]\n",
            "Generating train split: 27274 examples [00:37, 946.16 examples/s]\n",
            " 52% 27298/52002 [00:20<00:26, 920.49it/s]\u001b[A\n",
            "Generating train split: 27402 examples [00:37, 911.71 examples/s]\n",
            "Generating train split: 27511 examples [00:37, 953.16 examples/s]\n",
            "Generating train split: 27644 examples [00:38, 925.38 examples/s]\n",
            "Generating train split: 27746 examples [00:38, 942.76 examples/s]\n",
            "Generating train split: 27890 examples [00:38, 946.29 examples/s]\n",
            " 54% 27896/52002 [00:21<00:25, 959.21it/s]\u001b[A\n",
            "Generating train split: 28014 examples [00:38, 902.95 examples/s]\n",
            "Generating train split: 28114 examples [00:38, 918.66 examples/s]\n",
            "Generating train split: 28215 examples [00:38, 941.64 examples/s]\n",
            "Generating train split: 28367 examples [00:38, 1093.64 examples/s]\n",
            "Generating train split: 28533 examples [00:38, 1243.67 examples/s]\n",
            "Generating train split: 28706 examples [00:38, 1377.21 examples/s]\n",
            "Generating train split: 28887 examples [00:39, 1498.91 examples/s]\n",
            "Generating train split: 29055 examples [00:39, 1549.61 examples/s]\n",
            "Generating train split: 29228 examples [00:39, 1599.96 examples/s]\n",
            "Generating train split: 29412 examples [00:39, 1668.80 examples/s]\n",
            "Generating train split: 29582 examples [00:39, 1676.33 examples/s]\n",
            "Generating train split: 29761 examples [00:39, 1701.71 examples/s]\n",
            "Generating train split: 29932 examples [00:39, 1699.06 examples/s]\n",
            "Generating train split: 30156 examples [00:39, 1615.63 examples/s]\n",
            "Generating train split: 30339 examples [00:39, 1668.29 examples/s]\n",
            "Generating train split: 30510 examples [00:39, 1672.91 examples/s]\n",
            "Generating train split: 30760 examples [00:40, 1662.86 examples/s]\n",
            "Generating train split: 30930 examples [00:40, 1671.43 examples/s]\n",
            " 60% 30951/52002 [00:22<00:12, 1697.18it/s]\u001b[A\n",
            "Generating train split: 31177 examples [00:40, 1658.83 examples/s]\n",
            "Generating train split: 31358 examples [00:40, 1695.40 examples/s]\n",
            "Generating train split: 31543 examples [00:40, 1735.62 examples/s]\n",
            "Generating train split: 31735 examples [00:40, 1784.87 examples/s]\n",
            "Generating train split: 31923 examples [00:40, 1809.48 examples/s]\n",
            "Generating train split: 32172 examples [00:40, 1751.13 examples/s]\n",
            " 62% 32233/52002 [00:23<00:11, 1751.15it/s]\u001b[A\n",
            "Generating train split: 32436 examples [00:41, 1750.72 examples/s]\n",
            "Generating train split: 32630 examples [00:41, 1794.11 examples/s]\n",
            "Generating train split: 32885 examples [00:41, 1756.11 examples/s]\n",
            " 63% 32962/52002 [00:24<00:10, 1770.03it/s]\u001b[A\n",
            "Generating train split: 33146 examples [00:41, 1747.31 examples/s]\n",
            "Generating train split: 33411 examples [00:41, 1750.48 examples/s]\n",
            "Generating train split: 33595 examples [00:41, 1769.83 examples/s]\n",
            " 65% 33671/52002 [00:24<00:10, 1752.83it/s]\u001b[A\n",
            "Generating train split: 33850 examples [00:41, 1739.28 examples/s]\n",
            "Generating train split: 34080 examples [00:42, 1667.10 examples/s]\n",
            "Generating train split: 34252 examples [00:42, 1676.02 examples/s]\n",
            "Generating train split: 34428 examples [00:42, 1692.75 examples/s]\n",
            "Generating train split: 34603 examples [00:42, 1703.77 examples/s]\n",
            "Generating train split: 34779 examples [00:42, 1715.37 examples/s]\n",
            "Generating train split: 35018 examples [00:42, 1662.54 examples/s]\n",
            "Generating train split: 35190 examples [00:42, 1674.75 examples/s]\n",
            " 68% 35247/52002 [00:25<00:09, 1680.59it/s]\u001b[A\n",
            "Generating train split: 35431 examples [00:42, 1644.70 examples/s]\n",
            "Generating train split: 35607 examples [00:42, 1672.04 examples/s]\n",
            "Generating train split: 35783 examples [00:43, 1691.91 examples/s]\n",
            "Generating train split: 36000 examples [00:43, 1571.09 examples/s]\n",
            "Generating train split: 36172 examples [00:43, 1606.22 examples/s]\n",
            "Generating train split: 36343 examples [00:43, 1631.57 examples/s]\n",
            "Generating train split: 36579 examples [00:43, 1602.00 examples/s]\n",
            "Generating train split: 36760 examples [00:43, 1653.33 examples/s]\n",
            "Generating train split: 36947 examples [00:43, 1707.57 examples/s]\n",
            " 71% 36966/52002 [00:26<00:08, 1709.17it/s]\u001b[A\n",
            "Generating train split: 37184 examples [00:43, 1656.44 examples/s]\n",
            "Generating train split: 37361 examples [00:44, 1684.25 examples/s]\n",
            "Generating train split: 37532 examples [00:44, 1686.18 examples/s]\n",
            "Generating train split: 37792 examples [00:44, 1700.69 examples/s]\n",
            "Generating train split: 37963 examples [00:44, 1699.05 examples/s]\n",
            "Generating train split: 38134 examples [00:44, 1700.11 examples/s]\n",
            "Generating train split: 38318 examples [00:44, 1733.68 examples/s]\n",
            " 74% 38381/52002 [00:27<00:07, 1719.81it/s]\u001b[A\n",
            "Generating train split: 38562 examples [00:44, 1686.41 examples/s]\n",
            "Generating train split: 38732 examples [00:44, 1685.81 examples/s]\n",
            "Generating train split: 38961 examples [00:44, 1623.27 examples/s]\n",
            "Generating train split: 39160 examples [00:45, 1518.80 examples/s]\n",
            "Generating train split: 39388 examples [00:45, 1516.11 examples/s]\n",
            "Generating train split: 39563 examples [00:45, 1569.01 examples/s]\n",
            " 76% 39569/52002 [00:28<00:07, 1577.27it/s]\u001b[A\n",
            "Generating train split: 39809 examples [00:45, 1587.15 examples/s]\n",
            "Generating train split: 39970 examples [00:45, 1590.35 examples/s]\n",
            "Generating train split: 40216 examples [00:45, 1602.81 examples/s]\n",
            " 77% 40224/52002 [00:28<00:07, 1623.35it/s]\u001b[A\n",
            "Generating train split: 40437 examples [00:45, 1556.60 examples/s]\n",
            "Generating train split: 40637 examples [00:46, 1656.75 examples/s]\n",
            "Generating train split: 40807 examples [00:46, 1666.60 examples/s]\n",
            "Generating train split: 40995 examples [00:46, 1720.25 examples/s]\n",
            "Generating train split: 41247 examples [00:46, 1702.32 examples/s]\n",
            " 79% 41270/52002 [00:29<00:06, 1708.29it/s]\u001b[A\n",
            "Generating train split: 41509 examples [00:46, 1712.49 examples/s]\n",
            "Generating train split: 41725 examples [00:46, 1619.27 examples/s]\n",
            "Generating train split: 41894 examples [00:46, 1633.60 examples/s]\n",
            " 81% 41964/52002 [00:29<00:06, 1643.80it/s]\u001b[A\n",
            "Generating train split: 42133 examples [00:46, 1617.50 examples/s]\n",
            "Generating train split: 42374 examples [00:47, 1609.59 examples/s]\n",
            "Generating train split: 42541 examples [00:47, 1621.11 examples/s]\n",
            "Generating train split: 42721 examples [00:47, 1661.87 examples/s]\n",
            "Generating train split: 42944 examples [00:47, 1598.93 examples/s]\n",
            "Generating train split: 43106 examples [00:47, 1602.18 examples/s]\n",
            " 83% 43126/52002 [00:30<00:05, 1607.46it/s]\u001b[A\n",
            "Generating train split: 43338 examples [00:47, 1576.53 examples/s]\n",
            "Generating train split: 43515 examples [00:47, 1621.44 examples/s]\n",
            "Generating train split: 43683 examples [00:47, 1632.60 examples/s]\n",
            "Generating train split: 43859 examples [00:47, 1664.54 examples/s]\n",
            "Generating train split: 44114 examples [00:48, 1673.94 examples/s]\n",
            "Generating train split: 44296 examples [00:48, 1708.87 examples/s]\n",
            " 85% 44341/52002 [00:31<00:04, 1711.47it/s]\u001b[A\n",
            "Generating train split: 44546 examples [00:48, 1691.46 examples/s]\n",
            "Generating train split: 44729 examples [00:48, 1723.02 examples/s]\n",
            "Generating train split: 44907 examples [00:48, 1737.91 examples/s]\n",
            "Generating train split: 45110 examples [00:48, 1398.01 examples/s]\n",
            "Generating train split: 45301 examples [00:49, 1225.60 examples/s]\n",
            "Generating train split: 45452 examples [00:49, 1158.98 examples/s]\n",
            " 87% 45453/52002 [00:31<00:05, 1131.52it/s]\u001b[A\n",
            "Generating train split: 45621 examples [00:49, 1145.22 examples/s]\n",
            "Generating train split: 45788 examples [00:49, 1132.85 examples/s]\n",
            " 88% 45807/52002 [00:32<00:05, 1109.28it/s]\u001b[A\n",
            "Generating train split: 45926 examples [00:49, 1069.20 examples/s]\n",
            "Generating train split: 46049 examples [00:49, 992.62 examples/s] \n",
            "Generating train split: 46187 examples [00:49, 969.22 examples/s]\n",
            " 89% 46227/52002 [00:32<00:06, 943.27it/s]\u001b[A\n",
            "Generating train split: 46324 examples [00:50, 946.54 examples/s]\n",
            "Generating train split: 46423 examples [00:50, 954.79 examples/s]\n",
            "Generating train split: 46530 examples [00:50, 980.87 examples/s]\n",
            "Generating train split: 46659 examples [00:50, 938.68 examples/s]\n",
            "Generating train split: 46806 examples [00:50, 948.10 examples/s]\n",
            "Generating train split: 46904 examples [00:50, 952.32 examples/s]\n",
            "Generating train split: 47000 examples [00:50, 923.59 examples/s]\n",
            "Generating train split: 47111 examples [00:50, 964.74 examples/s]\n",
            " 91% 47119/52002 [00:33<00:05, 956.24it/s]\u001b[A\n",
            "Generating train split: 47237 examples [00:51, 917.19 examples/s]\n",
            "Generating train split: 47339 examples [00:51, 937.32 examples/s]\n",
            "Generating train split: 47435 examples [00:51, 939.10 examples/s]\n",
            "Generating train split: 47570 examples [00:51, 920.90 examples/s]\n",
            "Generating train split: 47679 examples [00:51, 963.37 examples/s]\n",
            " 92% 47700/52002 [00:34<00:04, 960.14it/s]\u001b[A\n",
            "Generating train split: 47823 examples [00:51, 958.03 examples/s]\n",
            "Generating train split: 47952 examples [00:51, 921.22 examples/s]\n",
            "Generating train split: 48053 examples [00:51, 922.76 examples/s]\n",
            "Generating train split: 48154 examples [00:52, 941.30 examples/s]\n",
            "Generating train split: 48254 examples [00:52, 954.45 examples/s]\n",
            "Generating train split: 48381 examples [00:52, 912.54 examples/s]\n",
            "Generating train split: 48482 examples [00:52, 934.17 examples/s]\n",
            " 93% 48485/52002 [00:35<00:03, 928.68it/s]\u001b[A\n",
            "Generating train split: 48626 examples [00:52, 939.29 examples/s]\n",
            "Generating train split: 48722 examples [00:52, 942.24 examples/s]\n",
            "Generating train split: 48897 examples [00:52, 1152.35 examples/s]\n",
            "Generating train split: 49062 examples [00:52, 1281.69 examples/s]\n",
            "Generating train split: 49227 examples [00:52, 1383.36 examples/s]\n",
            "Generating train split: 49403 examples [00:53, 1485.14 examples/s]\n",
            "Generating train split: 49636 examples [00:53, 1503.63 examples/s]\n",
            "Generating train split: 49813 examples [00:53, 1569.16 examples/s]\n",
            " 96% 49813/52002 [00:36<00:01, 1586.70it/s]\u001b[A\n",
            "Generating train split: 49990 examples [00:53, 1618.92 examples/s]\n",
            "Generating train split: 50229 examples [00:53, 1601.44 examples/s]\n",
            "Generating train split: 50398 examples [00:53, 1615.02 examples/s]\n",
            "Generating train split: 50580 examples [00:53, 1665.66 examples/s]\n",
            "Generating train split: 50839 examples [00:53, 1680.14 examples/s]\n",
            " 98% 50858/52002 [00:36<00:00, 1726.55it/s]\u001b[A\n",
            "Generating train split: 51095 examples [00:54, 1663.00 examples/s]\n",
            "Generating train split: 51343 examples [00:54, 1657.89 examples/s]\n",
            "Generating train split: 51511 examples [00:54, 1659.66 examples/s]\n",
            "Generating train split: 51689 examples [00:54, 1689.21 examples/s]\n",
            "Generating train split: 51859 examples [00:54, 1691.44 examples/s]\n",
            "100% 52002/52002 [00:37<00:00, 1392.40it/s]\n",
            "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-eb6b821bd81cdfce/0.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "!python tokenize_dataset_rows.py \\\n",
        "    --jsonl_path data/alpaca_data.jsonl \\\n",
        "    --save_path data/alpaca \\\n",
        "    --max_seq_length 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Iy-Gmjj0zMe8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "1PeyLKgIzMe9",
        "outputId": "63b4b35b-2559-4370-d79f-5b2e4f34df92"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-05a7497b5dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodeling_chatglm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGLMForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'modeling_chatglm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
        "from modeling_chatglm import ChatGLMForConditionalGeneration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "\n",
        "\n",
        "model = ChatGLMForConditionalGeneration.from_pretrained(\"THUDM/chatglm-6b\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
        "model.supports_gradient_checkpointing = True\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkiQopI8zMe-",
        "outputId": "fa146e15-7a37-4019-9a84-91d8c6f2997b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOVvUENlzMe_"
      },
      "source": [
        "## Test before finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ExHJ2dUzMfB",
        "outputId": "081a8742-6fc4-48dd-b2fc-840d9d1b1748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Give three tips for staying healthy.\n",
            "Answer: I'm sorry, but I'm not sure what you're asking for. Could you please provide more context or clarify your question?\n",
            "### 1.Answer:\n",
            " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule. \n",
            "\n",
            "\n",
            "Instruction: What are the three primary colors?\n",
            "Answer: The three primary colors in painting are red, blue, and green. These colors are often used in combination to create more complex and vibrant colors.\n",
            "### 2.Answer:\n",
            " The three primary colors are red, blue, and yellow. \n",
            "\n",
            "\n",
            "Instruction: Describe the structure of an atom.\n",
            "Answer: The原子是构成物质的基本单位,由带正电荷的质子和不带电荷的电子组成。原子核由带正电荷的质子和不带电荷的中子组成,它们之间通过核力相互吸引。\n",
            "\n",
            "原子的化学性质取决于其组成和结构,以及化学反应中所涉及的因素。例如,氧原子是化学反应中最常见的原子之一,因为它具有两个电子,可以与许多其他原子形成化合物。\n",
            "\n",
            "物质是由原子和分子组成的,而原子是物质的基本单位。了解原子的结构、性质以及化学反应,可以帮助我们更好地理解物质世界,并更好地利用这些物质来制造产品、治疗疾病。\n",
            "### 3.Answer:\n",
            " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
            "\n",
            "\n",
            "Instruction: How can we reduce air pollution?\n",
            "Answer: I'm sorry, but I'm not sure what you're asking. Could you please provide more context or clarify your question?\n",
            "### 4.Answer:\n",
            " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
            "\n",
            "\n",
            "Instruction: Describe a time when you had to make a difficult decision.\n",
            "Answer: I'm sorry, but I'm not sure what you're asking for. Could you please provide more context or clarify your question?\n",
            "### 5.Answer:\n",
            " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from cover_alpaca2jsonl import format_example\n",
        "import json\n",
        "\n",
        "\n",
        "instructions = json.load(open(\"data/alpaca_data.json\"))\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, item in enumerate(instructions[:5]):\n",
        "        feature = format_example(item)\n",
        "        input_text = feature[\"context\"]\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=150,\n",
        "            temperature=0\n",
        "        )\n",
        "        answer = tokenizer.decode(out[0])\n",
        "        print(answer)\n",
        "        item['infer_answer'] = answer\n",
        "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1_wbgKTzMfC"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32, lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.is_parallelizable = True\n",
        "model.model_parallel = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZc0sE4izMfC"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "dataset_path = \"data/alpaca/\"\n",
        "\n",
        "dataset = datasets.load_from_disk(dataset_path)\n",
        "\n",
        "train_num = 500\n",
        "\n",
        "mini_train_dataset = datasets.Dataset.from_dict(dataset[:train_num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiyePx_bzMfD"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, HfArgumentParser\n",
        "\n",
        "\n",
        "def get_masks_and_position_ids(\n",
        "    seq, seq_len, context_length, device, gmask=False, position_encoding_2d=True\n",
        "):\n",
        "    mask_position = (\n",
        "        seq_len - 2\n",
        "    )  # is equal to `seq.index(mask_token)` or `seq.index(150001)`\n",
        "    attention_mask = torch.ones((1, context_length, context_length), device=device)\n",
        "    attention_mask.tril_()\n",
        "    attention_mask[..., : mask_position - 1] = 1\n",
        "    attention_mask = (attention_mask < 0.5).bool()\n",
        "\n",
        "    if position_encoding_2d:\n",
        "        seq_length = seq_len - 1  # is equal to `seq_length = seq.index(150004)`\n",
        "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
        "        if not gmask:\n",
        "            position_ids[seq_length:] = mask_position\n",
        "        block_position_ids = torch.cat(\n",
        "            (\n",
        "                torch.zeros(seq_length, dtype=torch.long, device=device),\n",
        "                torch.arange(\n",
        "                    context_length - seq_length, dtype=torch.long, device=device\n",
        "                )\n",
        "                + 1,\n",
        "            )\n",
        "        )\n",
        "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
        "    else:\n",
        "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
        "        if not gmask:\n",
        "            position_ids[context_length - 1 :] = mask_position\n",
        "    return attention_mask, position_ids\n",
        "\n",
        "\n",
        "def data_collator(features: list) -> dict:\n",
        "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
        "    longest = max(len_ids)\n",
        "    input_ids = []\n",
        "    attention_mask_list = []\n",
        "    position_ids_list = []\n",
        "    labels_list = []\n",
        "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
        "        ids = feature[\"input_ids\"]\n",
        "        seq_len = feature[\"seq_len\"]\n",
        "        labels = (\n",
        "            [-100] * (seq_len - 1)\n",
        "            + ids[(seq_len - 1) :]\n",
        "            + [-100] * (longest - ids_l)\n",
        "        )\n",
        "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
        "        _ids = torch.LongTensor(ids)\n",
        "        attention_mask, position_ids = get_masks_and_position_ids(\n",
        "            ids, seq_len, longest, _ids.device, gmask=False\n",
        "        )\n",
        "        labels_list.append(torch.LongTensor(labels))\n",
        "        input_ids.append(_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "        position_ids_list.append(position_ids)\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    labels = torch.stack(labels_list)\n",
        "    attention_mask = torch.stack(attention_mask_list)\n",
        "    position_ids = torch.stack(position_ids_list)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": labels,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"position_ids\": position_ids,\n",
        "    }\n",
        "\n",
        "\n",
        "class ModifiedTrainer(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        return model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            position_ids=inputs[\"position_ids\"],\n",
        "            labels=inputs[\"labels\"],\n",
        "        ).loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gv-tATfzMfE",
        "outputId": "e55833b0-2124-4e9c-8071-3547af36d60d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 07:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.091900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.787700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.978900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.534500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.611100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.620300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.778000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.493800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.479100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.205300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.375400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.489700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.440500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.468200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.137700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.942300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.184100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.970900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.262800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.108000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.265900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>1.715500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.326100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=1.4622032979329427, metrics={'train_runtime': 474.9934, 'train_samples_per_second': 3.158, 'train_steps_per_second': 3.158, 'total_flos': 3781851053211648.0, 'train_loss': 1.4622032979329427, 'epoch': 3.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"output\",\n",
        "    fp16 =True,\n",
        "    gradient_accumulation_steps=1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    learning_rate = 1e-4,\n",
        "    max_steps=1500,\n",
        "    logging_steps=50,\n",
        "    remove_unused_columns=False,\n",
        "    seed=0,\n",
        "    data_seed=0,\n",
        "    group_by_length=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = ModifiedTrainer(\n",
        "    model=model,\n",
        "    train_dataset=mini_train_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAQaD1lmzMfF"
      },
      "source": [
        "## Test After finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvLv3HcLzMfG",
        "outputId": "3852c887-be3d-4799-c69e-1498984abef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/transformers-4.27.0.dev0-py3.8.egg/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Give three tips for staying healthy.\n",
            "Answer: 1. Eat a balanced diet. \n",
            "2. Get regular exercise. \n",
            "3. Stay hydrated by drinking plenty of water.\n",
            "### 1.Answer:\n",
            " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule. \n",
            "\n",
            "\n",
            "Instruction: What are the three primary colors?\n",
            "Answer: The three primary colors are red, blue, and yellow.\n",
            "### 2.Answer:\n",
            " The three primary colors are red, blue, and yellow. \n",
            "\n",
            "\n",
            "Instruction: Describe the structure of an atom.\n",
            "Answer: An atom is a small particle of a chemical element, with a central electron surrounded by other electrons, which form a cloud around the central electron. The cloud of electrons is surrounded by a cloud of positive ions, which make up the原子's positive charge. The positive ions and negative ions are attracted to each other, and the atoms form a cloud of ions and electrons.\n",
            "### 3.Answer:\n",
            " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
            "\n",
            "\n",
            "Instruction: How can we reduce air pollution?\n",
            "Answer: There are several ways to reduce air pollution, including reducing energy consumption, improving transportation, reducing waste and reducing the use of harmful chemicals. Additionally, increasing public awareness and education, implementing policies, and increasing funding for research can also help reduce air pollution.\n",
            "### 4.Answer:\n",
            " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
            "\n",
            "\n",
            "Instruction: Describe a time when you had to make a difficult decision.\n",
            "Answer: I had to make a difficult decision when I was in my 20s. I had to choose between my career and my studies. I knew that my studies were more important, but I also knew that I wanted to be a teacher. I had to make a decision based on my values and my future plans. I decided to pursue my studies and became a teacher. It was a difficult decision, but I was determined to make the right choice.\n",
            "### 5.Answer:\n",
            " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from cover_alpaca2jsonl import format_example\n",
        "import json\n",
        "\n",
        "\n",
        "instructions = json.load(open(\"data/alpaca_data.json\"))\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, item in enumerate(instructions[:5]):\n",
        "        feature = format_example(item)\n",
        "        input_text = feature[\"context\"]\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=150,\n",
        "            temperature=0\n",
        "        )\n",
        "        answer = tokenizer.decode(out[0])\n",
        "        print(answer)\n",
        "        item['infer_answer'] = answer\n",
        "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njkMwhXyzMfG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def save_tunable_parameters(model, path):\n",
        "    saved_params = {\n",
        "        k: v.to(\"cpu\")\n",
        "        for k, v in model.named_parameters()\n",
        "        if v.requires_grad\n",
        "    }\n",
        "    torch.save(saved_params, path)\n",
        "\n",
        "\n",
        "save_tunable_parameters(model, os.path.join(\"output\", \"chatglm-lora.pt\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}